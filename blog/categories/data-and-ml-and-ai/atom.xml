<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[catogories：Data&ML&AI | Steven's Blog]]></title>
  <link href="http://isunix.github.io/blog/categories/data-and-ml-and-ai/atom.xml" rel="self"/>
  <link href="http://isunix.github.io/"/>
  <updated>2019-08-14T18:35:42+08:00</updated>
  <id>http://isunix.github.io/</id>
  <author>
    <name><![CDATA[Steven Sun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Data Science at the Command Line]]></title>
    <link href="http://isunix.github.io/blog/2019/08/14/data-science-at-the-command-line/"/>
    <updated>2019-08-14T14:13:16+08:00</updated>
    <id>http://isunix.github.io/blog/2019/08/14/data-science-at-the-command-line</id>
    <content type="html"><![CDATA[<p>本文是关于如何使用命令行的方式，来更好的认识你的数据.</p>

<p>参考链接:</p>

<ul>
  <li>
    <p><a href="https://csvkit.readthedocs.io/en/latest/tutorial/1_getting_started.html#installing-csvkit">csvkit</a></p>
  </li>
  <li>
    <p><a href="[https://www.datascienceatthecommandline.com](https://www.datascienceatthecommandline.com/)">Data Science at the Command Line</a></p>
  </li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gradient Boosting Algorithm]]></title>
    <link href="http://isunix.github.io/blog/2019/08/14/gradient-boosting-algorithm/"/>
    <updated>2019-08-14T13:51:36+08:00</updated>
    <id>http://isunix.github.io/blog/2019/08/14/gradient-boosting-algorithm</id>
    <content type="html"><![CDATA[<p>Gradient Boosting Algorithm 算法参考链接：</p>

<ul>
  <li>
    <p><a href="https://arxiv.org/pdf/1603.02754v1.pdf">XGBoost: A Scalable Tree Boosting System</a></p>
  </li>
  <li>
    <p><a href="https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/">Which algorithm takes the crown: Light GBM vs XGBOOST?</a></p>
  </li>
  <li>
    <p><a href="https://www.analyticsvidhya.com/blog/2015/09/complete-guide-boosting-methods/">Learn Gradient Boosting Algorithm for better predictions (with codes in R)</a></p>
  </li>
  <li>
    <p><a href="https://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/">Getting smart with Machine Learning – AdaBoost and Gradient Boost</a></p>
  </li>
  <li>
    <p><a href="https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/">Quick Introduction to Boosting Algorithms in Machine Learning</a></p>
  </li>
  <li><a href="https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/">Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python</a></li>
  <li>
    <p><a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">Complete Guide to Parameter Tuning in XGBoost with codes in Python</a></p>
  </li>
  <li>
    <p><a href="https://segmentfault.com/a/1190000014040317">XGboost数据比赛实战之调参篇</a></p>
  </li>
  <li><a href="http://www.sohu.com/a/226265476_609569">通俗的将Xgboost的原理讲明白</a></li>
  <li><a href="https://blog.csdn.net/app_12062011/article/details/52136117">决策树</a> 和 <a href="https://blog.csdn.net/zhaocj/article/details/50503450">决策树源码剖析</a></li>
  <li><a href="[http://matafight.github.io/2017/03/14/XGBoost-%E7%AE%80%E4%BB%8B/](http://matafight.github.io/2017/03/14/XGBoost-简介/)">XGBoost 笔记</a></li>
  <li>
    <p><a href="https://blog.csdn.net/liuzonghao88/article/details/88808408">XGBoost如何避免过拟合</a></p>
  </li>
  <li><a href="https://blog.csdn.net/u012735708/article/details/83651832">XGBoost调参笔记</a></li>
  <li><a href="https://blog.csdn.net/a358463121/article/details/68617389">xgboost中的数学原理</a></li>
  <li><a href="https://blog.csdn.net/sb19931201/article/details/52557382">xgboost入门与实战</a></li>
  <li><a href="https://blog.csdn.net/xiaocong1990/article/details/55107239">XGBoost参数调优</a></li>
  <li><a href="https://blog.csdn.net/Totoro1745/article/details/53328725">xgboost：一个纯小白的学习历程</a></li>
  <li>
    <p><a href="https://blog.csdn.net/github_38414650/article/details/76061893">通俗、有逻辑的写一篇说下Xgboost的原理</a></p>
  </li>
  <li>
    <p><a href="https://towardsdatascience.com/boosting-algorithm-gbm-97737c63daa3">Boosting algorithm: GBM</a></p>
  </li>
  <li><a href="https://www.imooc.com/article/43784?block_id=tuijian_wz"><strong>LightGBM 调参方法</strong></a></li>
  <li><a href="https://www.kaggle.com/c/LANL-Earthquake-Prediction/discussion/89909">Good summary of XGBoost vs CatBoost vs LightGBM</a></li>
  <li><a href="https://www.kaggle.com/samratp/lightgbm-xgboost-catboost">LightGBM + XGBoost + Catboost</a></li>
  <li><a href="https://datascience.stackexchange.com/questions/49567/lightgbm-vs-xgboost-vs-catboost">lightgbm-vs-xgboost-vs-catboost</a></li>
  <li><a href="https://medium.com/kaggle-nyc/gradient-boosting-decision-trees-xgboost-vs-lightgbm-and-catboost-72df6979e0bb">Gradient Boosting Decision trees: XGBoost vs LightGBM (and catboost)</a></li>
  <li>
    <p><a href="https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db">CatBoost vs. Light GBM vs. XGBoost</a></p>
  </li>
  <li><a href="https://blog.csdn.net/LrS62520kV/article/details/79620615">CatBoost vs. Light GBM vs. XGBoost 的中文翻译</a></li>
  <li><a href="https://stackoverflow.com/questions/44937698/lightgbm-oserror-library-not-loaded">关于lightgbm的安装</a></li>
  <li><a href="https://towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d">Custom Loss Functions for Gradient Boosting</a></li>
  <li><a href="https://github.com/Microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions">machine-learning-challenge-winning-solutions-lightgbm-winned</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于P值]]></title>
    <link href="http://isunix.github.io/blog/2019/08/07/guan-yu-pzhi/"/>
    <updated>2019-08-07T13:45:59+08:00</updated>
    <id>http://isunix.github.io/blog/2019/08/07/guan-yu-pzhi</id>
    <content type="html"><![CDATA[<p>参考链接:</p>

<p><a href="https://towardsdatascience.com/p-values-explained-by-data-scientist-f40a746cfc8">p值是什么？数据科学家用最简单的方式告诉你</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[关于正态分布还有偏度与峰度]]></title>
    <link href="http://isunix.github.io/blog/2019/08/02/guan-yu-zheng-tai-fen-bu-huan-you-pian-du-yu-feng-du/"/>
    <updated>2019-08-02T08:29:35+08:00</updated>
    <id>http://isunix.github.io/blog/2019/08/02/guan-yu-zheng-tai-fen-bu-huan-you-pian-du-yu-feng-du</id>
    <content type="html"><![CDATA[
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Spark的MLlib进行机器学习]]></title>
    <link href="http://isunix.github.io/blog/2019/08/01/shi-yong-sparkde-mllibjin-xing-ji-qi-xue-xi/"/>
    <updated>2019-08-01T17:55:54+08:00</updated>
    <id>http://isunix.github.io/blog/2019/08/01/shi-yong-sparkde-mllibjin-xing-ji-qi-xue-xi</id>
    <content type="html"><![CDATA[<p>1.比如我们有个 case class</p>

<p><code>scala
case class CaseClassTest(user: String, name: String)
</code></p>

<p>我们通过使用<code>mapPartitions</code>来进行操作， 注意此处<code>mapPartitions</code>的用法:</p>

<p><code>scala
val mapResult = spark.read.textFile(input).map(_.split("\t")).mapPartitions {
      iter =&gt;
        iter.map {
          iterms =&gt; CaseClassTest(iterms(0), iterms(1))
        }
    }
</code></p>

<p>2.reduceByKey<code>  和 </code>case` 的用法</p>

<p><code>scala
val data = gowalla.map {
      check: CheckIn =&gt; (check.user, (1L, Set(check.time), Set(check.location)))
    }.rdd.reduceByKey{
      case (left, right) =&gt; (left._1 + right._1, left._2.union(right._2),left._3.union(right._3))
    }.map{
      case (_, (checkins, days:Set[String], locations:Set[String])) =&gt;
        Vectors.dense(checkins.toDouble, days.size.toDouble, locations.size.toDouble)
        //次数，天数，地点数
    }
</code></p>

<p><code>reduceByKey</code> 这里是求 <code>(1L, Set(check.time), Set(check.location))</code> 中的一个元素累加，第二个元素求并集，第三个元素也是求并集. <code>case</code> 的作用是让输入符合规范； 注意在 <code>map</code> 中，我们可以限定指示变量的类型， 比如这里的 <code>check: CheckIn =&gt;</code></p>

<h4 id="section">参考</h4>

<ol>
  <li>Spark机器学习进阶实战</li>
</ol>
]]></content>
  </entry>
  
</feed>
