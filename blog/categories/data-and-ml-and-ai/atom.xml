<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[catogories：Data&ML&AI | Steven's Blog]]></title>
  <link href="http://isunix.github.io/blog/categories/data-and-ml-and-ai/atom.xml" rel="self"/>
  <link href="http://isunix.github.io/"/>
  <updated>2020-12-06T15:15:18+08:00</updated>
  <id>http://isunix.github.io/</id>
  <author>
    <name><![CDATA[Steven Sun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[单纯形法介绍和Python代码实现]]></title>
    <link href="http://isunix.github.io/blog/2020/11/30/dan-chun-xing-fa-jie-shao-he-pythondai-ma-shi-xian/"/>
    <updated>2020-11-30T09:31:44+08:00</updated>
    <id>http://isunix.github.io/blog/2020/11/30/dan-chun-xing-fa-jie-shao-he-pythondai-ma-shi-xian</id>
    <content type="html"><![CDATA[<h1 id="section">参考链接:</h1>
<ul>
  <li><a href="https://zhuanlan.zhihu.com/p/31644892">简单理解线性规划的单纯形算法</a></li>
  <li><a href="https://jeremykun.com/2014/06/02/linear-programming-and-the-most-affordable-healthy-diet-part-1/">Linear Programming and Healthy Diets
</a></li>
  <li><a href="https://jeremykun.com/2014/12/01/linear-programming-and-the-simplex-algorithm/">Linear Programming and the Simplex Algorithm
</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[文本主题挖掘浅尝辄止]]></title>
    <link href="http://isunix.github.io/blog/2020/11/24/wen-ben-zhu-ti-wa-jue-qian-chang-zhe-zhi/"/>
    <updated>2020-11-24T17:32:37+08:00</updated>
    <id>http://isunix.github.io/blog/2020/11/24/wen-ben-zhu-ti-wa-jue-qian-chang-zhe-zhi</id>
    <content type="html"><![CDATA[<h2 id="section">关键词:</h2>

<p><code>向量</code>, <code>分布</code>, <code>期望</code>, <code>隐含狄利克雷分布</code>, <code>贝叶斯模型</code>, <code>先验分布</code>, <code>似然</code>, <code>后验分布</code>, <code>二项分布</code>, <code>Beta分布</code>, <code>Gamma函数</code>, <code>多项分布</code>, <code>Dirichlet 分布</code>, <code>文档</code>, <code>词</code>, <code>主题</code>, <code>超参数</code>, <code>Gibbs采样</code>, <code>变分推断EM</code>, <code>联合分布</code>, <code>条件分布</code>, <code>状态转移矩阵</code>, <code>马尔科夫链</code>, <code>平稳</code>, <code>二维正态分布</code>,  <code>变分推断</code>, <code>EM算法</code>, <code>KL散度</code>, <code>KL距离</code>, <code>交叉熵</code>, <code>变分分布</code>, <code>Jensen不等式</code>, <code>凸函数</code>, <code>正则</code>, <code>分词</code>, <code>bow(bag of word)</code>, <code>相似度度量</code></p>

<h2 id="section-1">代码:</h2>

<p>```python
import re
import sys
import jieba</p>

<p>user_dict_file = “my_dict_watch_77.csv”
jieba.load_userdict(user_dict_file)</p>

<p>stop_words_file = “hit_stopwords.txt”
stopwords = open(stop_words_file, ‘r’, encoding = ‘utf8’).readlines()
stopwords = [w.strip() for w in stopwords]</p>

<p>file_to_process = “watch_77.csv”</p>

<p>def word_cut(input_file):
    result_arr = []</p>

<pre><code>with open(file_to_process, 'r') as f:
for line in f.readlines():
    line = line.strip()
    if len(line) &lt; 3:
        continue

    out_str = ""
    re_chinese = re.compile(u"[\u4e00-\u9fa5]+")
    w = re.sub(r'[A-Za-z0-9]|\d+', '', str(line)) # removed chars, but what about "app"? 
    seg_list = jieba.lcut(w, cut_all=False)

    for word in seg_list:
        if word not in stopwords and word != '\t' and re_chinese.search(word, 0) and len(word.strip()) &gt;= 2:
            out_str += word
            out_str += " "

    result_arr.append(out_str.strip().split(" "))
return result_arr
</code></pre>

<p>seg_out_file = “out_file_20201124.txt”
result_arr = word_cut(file_to_process)</p>

<p>with open(seg_out_file, ‘w’,encoding = ‘utf-8’) as file:
        file.write(str(result_arr))</p>

<p>from jieba import analyse
tfidf = analyse.extract_tags</p>

<p>with open(seg_out_file, ‘r’, encoding=’utf-8’) as file:
    texts = file.readlines()</p>

<p>keywords = jieba.analyse.extract_tags(str(texts),
                                      topK=150,
                                      withWeight=True,
                                      allowPOS=(‘nr’, ‘ns’, ‘nt’, ‘nz’, ‘n’, ‘vn’, ‘v’))
for item in keywords:
    print(item)</p>

<p>from gensim import corpora, models, similarities
from gensim.models import LdaModel
from gensim.corpora import Dictionary
import pyLDAvis
import pyLDAvis.gensim</p>

<p>dictionary = corpora.Dictionary(result_arr)
corpus = [dictionary.doc2bow(text) for text in result_arr]</p>

<p>lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=10)
lda.print_topics(2)</p>

<p>vis_data = pyLDAvis.gensim.prepare(lda, corpus, dictionary)
pyLDAvis.display(vis_data)
```</p>

<h2 id="section-2">参考:</h2>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">LDA wiki</a></li>
  <li><a href="https://juejin.cn/post/6844904094771970056">文本挖掘之LDA主题模型</a></li>
  <li><a href="https://scikit-learn.org/dev/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py">LDA sk-learn</a></li>
  <li><a href="https://github.com/Nitro/scalda">LDA Scala</a></li>
  <li><a href="https://github.com/XiaoMi/MiNLP/tree/main/minlp-tokenizer">MiNLP-Tokenizer</a></li>
  <li><a href="https://www.cnblogs.com/pinard/p/6831308.html">文本主题模型之LDA(一) LDA基础： 刘建平</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/263065290">文本主题模型之LDA</a></li>
  <li><a href="https://github.com/isnowfy/snownlp">snownlp</a></li>
  <li><a href="https://www.cnblogs.com/Luv-GEM/p/10881838.html">文本主题抽取：用gensim训练LDA模型</a></li>
  <li><a href="https://github.com/fxsjy/jieba">jieba</a></li>
  <li><a href="https://github.com/lancopku/pkuseg-python">pkuseg-python</a></li>
  <li><a href="https://github.com/kimmy-sil/Python-beginning-practice">中文分词处理</a></li>
  <li><a href="https://blog.csdn.net/qq_16633405/article/details/80578804">Doc2Bow简介与实践Demo</a></li>
  <li><a href="https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/">Beginners Guide to Topic Modeling in Python</a></li>
  <li><a href="https://blog.csdn.net/selinda001/article/details/80446766">主题模型 LDA 入门（附 Python 代码）</a></li>
  <li><a href="https://github.com/goto456/stopwords">中文常用停用词表</a></li>
  <li><a href="https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24">Topic Modeling and Latent Dirichlet Allocation (LDA) in Python</a></li>
  <li><a href="https://blog.csdn.net/vs412237401/article/details/52238248">用docsim/doc2vec/LSH比较两个文档之间的相似度</a></li>
  <li><a href="https://github.com/amueller/word_cloud">wordcloud</a></li>
  <li><a href="https://github.com/bmabey/pyLDAvis">pyLDAvis</a></li>
  <li><a href="https://blog.csdn.net/v\_july\_v/article/details/41209515">通俗理解LDA主题模型
</a></li>
  <li><a href="https://www.eecis.udel.edu/\~shatkay/Course/papers/UIntrotoTopicModelsBlei2011-5.pdf">Introduction to Probabilistic Topic Models
</a></li>
  <li><a href="https://github.com/nkhuyu/python-LDA">python-LDA Git Code</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SparkSQL入门和进阶]]></title>
    <link href="http://isunix.github.io/blog/2020/11/20/sparksqlru-men-he-jin-jie/"/>
    <updated>2020-11-20T22:08:55+08:00</updated>
    <id>http://isunix.github.io/blog/2020/11/20/sparksqlru-men-he-jin-jie</id>
    <content type="html"><![CDATA[<h2 id="section">常用的函数：</h2>

<p><code>select</code>、<code>lit</code>、<code>as</code>、<code>groupBy</code>、<code>agg</code>、<code>sum</code>、<code>as</code>、<code>where</code>、<code>withColumn</code>、 <code>col</code>、 <code>when</code>、 <code>otherwise</code>、 <code>join</code>、<code>withColumnRenamed</code>、 <code>isin</code>、 <code>cast</code>、 <code>$</code>、 <code>union</code>、 <code>gt</code>、 <code>struct</code>、<code>sort</code>、<code>desc</code>、<code>show</code>、<code>orderBy</code>、<code>asc</code>、<code>repartition</code>、<code>sortWithinPartitions</code>、<code>filter</code>、<code>selectExpr</code>、<code>pivot</code>、<code>expr</code>、<code>row_number</code>、<code>over</code>、 <code>partitionBy</code></p>

<p>```scala
df
.select($”id”,lit(1).as(“cnt”)) 
.groupBy(“idd”) 
.agg(sum(“cnt”).as(“total”))
.where(“total &gt;=” + cnt2) 
.select(“uid”,”total”)</p>

<p>```</p>

<h2 id="udf">UDF：</h2>

<p><code>spark.udf.register
</code></p>

<h2 id="section-1">参考链接:</h2>
<ul>
  <li><a href="https://blog.csdn.net/weixin_40652340/article/details/79207455">SparkSQL DSL开发
</a></li>
  <li><a href="http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Column.html">Column</a></li>
  <li><a href="http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html">Dataset</a></li>
  <li><a href="http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions\$.html">functions</a></li>
  <li><a href="http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/index.html\#DataFrame=org.apache.spark.sql.Dataset\[org.apache.spark.sql.Row\]">sql</a></li>
  <li><a href="https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html">Scalar User Defined Functions (UDFs)
</a></li>
  <li><a href="https://www.cnblogs.com/Diyo/p/11410895.html">SparkSQL DSL 随便写写</a></li>
  <li><a href="https://blog.csdn.net/weixin_42419342/article/details/108918139?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~sobaiduend\~default-2-108918139.nonecase&amp;utm_term=sparksql中dsl&amp;spm=1000.2123.3001.4430">SparkSQL案例——用SQL和DSL两种语法格式，求出用户连续登录天数
</a></li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[一些关于RNN和LSTM和GRU的阅读材料]]></title>
    <link href="http://isunix.github.io/blog/2019/08/27/%5B%3F%5D-xie-guan-yu-rnnhe-lstmhe-grude-yue-du-cai-liao/"/>
    <updated>2019-08-27T15:40:38+08:00</updated>
    <id>http://isunix.github.io/blog/2019/08/27/[?]-xie-guan-yu-rnnhe-lstmhe-grude-yue-du-cai-liao</id>
    <content type="html"><![CDATA[<p><a href="https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9">Illustrated Guide to Recurrent Neural Networks</a></p>

<p><a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">Illustrated Guide to LSTM’s and GRU’s: A step by step explanation</a></p>

<p><a href="https://zhuanlan.zhihu.com/p/37204589">直觉理解LSTM和GRU</a></p>

<p><a href="https://zhuanlan.zhihu.com/p/28687529">RNN梯度消失和爆炸的原因</a></p>

<p><a href="https://zhuanlan.zhihu.com/p/28749444">LSTM如何解决梯度消失问题</a></p>

<p><a href="https://zhuanlan.zhihu.com/p/32481747">人人都能看懂的GRU</a></p>

<p><a href="https://zhuanlan.zhihu.com/p/32085405">人人都能看懂的LSTM</a></p>

<p><a href="http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Sequence_Models/循环序列模型">循环序列模型</a></p>

<p><a href="http://www.ai-start.com/dl2017/html/lesson5-week1.html">第五门课 序列模型(Sequence Models)</a></p>

<p><a href="https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/">Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras</a></p>

<p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></p>

<p><a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/">Implementing a GRU/LSTM RNN with Python and Theano</a></p>

<p><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">Introduction to RNNs</a></p>

<p><a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/">Implementing a Neural Network from Scratch in Python</a></p>

<p><a href="http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/">Speeding up your Neural Network with Theano and the GPU</a></p>

<p><a href="http://cs231n.github.io/optimization-2/">CS231 On Gradients And BackPropagation</a></p>

<p><a href="http://colah.github.io/posts/2015-08-Backprop/">Calculus on Computational Graphs: Backpropagation</a></p>

<p><a href="http://neuralnetworksanddeeplearning.com/chap2.html">How the backpropagation algorithm works</a></p>

<p><a href="http://cs231n.github.io/python-numpy-tutorial/">Numpy Tutorial From CS231</a></p>

<p><a href="https://www.leiphone.com/news/201908/wq1m4fNK8LNuG0Rs.html">卷积神经网络数学原理解析</a></p>

<p><a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark中排序输出]]></title>
    <link href="http://isunix.github.io/blog/2019/08/27/sparkzhong-pai-xu-shu-chu/"/>
    <updated>2019-08-27T10:11:02+08:00</updated>
    <id>http://isunix.github.io/blog/2019/08/27/sparkzhong-pai-xu-shu-chu</id>
    <content type="html"><![CDATA[<p>```python
# initialize pyspark
import pandas as pd
import numpy as np
import json
np.set_printoptions(suppress=True)</p>

<p>import findspark
findspark.init()
import pyspark</p>

<p>from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .appName(‘PySpark-Analysis’) \
    .config(“spark.executor.memory”, “3g”) \
    .config(“spark.executor.cores”, “8”) \
    .getOrCreate()</p>

<p>import os
folder = “xxxx”
filename = “one-big.tsv”
file = os.path.join(folder, filename)</p>

<p>df = spark.read.text(file).rdd.map(lambda r: r[0]).map(lambda line: line.split(“\t”)).toDF()</p>

<p>df.orderBy(“<em>1”, “</em>2”).coalesce(1).write.csv(“xxx2”, sep=’\t’)
```</p>

]]></content>
  </entry>
  
</feed>
