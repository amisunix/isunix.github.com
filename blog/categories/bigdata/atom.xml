<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[catogories：BigData | Steven's Blog]]></title>
  <link href="http://isunix.github.io/blog/categories/bigdata/atom.xml" rel="self"/>
  <link href="http://isunix.github.io/"/>
  <updated>2019-07-24T15:40:24+08:00</updated>
  <id>http://isunix.github.io/</id>
  <author>
    <name><![CDATA[Steven Sun]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Spark文件操作备忘]]></title>
    <link href="http://isunix.github.io/blog/2019/07/08/sparkwen-jian-cao-zuo-bei-wang/"/>
    <updated>2019-07-08T11:08:53+08:00</updated>
    <id>http://isunix.github.io/blog/2019/07/08/sparkwen-jian-cao-zuo-bei-wang</id>
    <content type="html"><![CDATA[<h3 id="repartition">读取一个文本文件，然后进行repartition:</h3>

<p><code>sh
spark.read.textFile(path1).repartition(1).write.text(path2)
</code></p>

<h3 id="pyspark-dataframe">pyspark 基本的dataframe的操作:</h3>

<p>```py
from pyspark.sql import SparkSession
master = “local”</p>

<p>df = spark.read.parquet(raw_data)
df_filtered = df.select(‘f1’, ‘f2’, ‘f3’, ‘f4’).filter((df[‘f1’] &gt;= 1531670400) &amp; (df[‘f1’] &lt;= 1532188800) &amp; (df[‘f2’] == 15) &amp; (df[‘f3’] != ‘’) &amp; (df[‘f4’] &gt; 0))</p>

<p>import pyspark.sql.functions as func
result = df_filtered.groupby(df[‘f3’]).agg(func.countDistinct(‘f1’))</p>

<p>result.rdd.repartition(1).map(lambda row: str(row[0]) + “,” + str(row[1])).saveAsTextFile(path2)
```</p>

<h3 id="spark">用Spark操作数据</h3>

<p>```scala
val diff = diff1.union(diff2)</p>

<p>val raw = spark.read.textFile(path1)</p>

<p>// 限定有3个字段, 存成df
val raw_df = raw.filter(x =&gt; x.split(“\t”).length == 3).map(x =&gt; (x.split(“\t”)(0), x.split(“\t”)(1).toLong, x.split(“\t”)(2))).toDF(“f1”, “f2”, “f3”)</p>

<p>// join获取rawdata
val joined_diff = diff.join(raw_df, diff(“f1”) === raw_df(“f1”) &amp;&amp; diff(“f2”) === raw_df(“f2”), “inner”).toDF(“f1”, “f2”, “f3”, “f4”, “f5”, “f6”)</p>

<p>// 存数据
val result_df = joined_diff.select($”f1”, $”f2”, $”f3”, $”f4”)
result_df.repartition(1).write.mode(“overwrite”).option(“delimiter”, “\t”).csv(path2)
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zeppelin搭配Presto]]></title>
    <link href="http://isunix.github.io/blog/2019/06/10/zeppelinda-pei-presto/"/>
    <updated>2019-06-10T17:57:13+08:00</updated>
    <id>http://isunix.github.io/blog/2019/06/10/zeppelinda-pei-presto</id>
    <content type="html"><![CDATA[<p>参考文章</p>

<p><code>https://stackoverflow.com/questions/35858606/presto-interpreter-in-zeppelin-on-emr</code></p>

<ul>
  <li>在 master 机器上安装 jdbc</li>
</ul>

<p><code>sh
sudo /usr/lib/zeppelin/bin/install-interpreter.sh --name jdbc
</code></p>

<p><code>sh
sudo stop zeppelin
sudo start zeppelin
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[EMR中创建HUE的superuser]]></title>
    <link href="http://isunix.github.io/blog/2019/06/10/emrzhong-chuang-jian-huede-superuser/"/>
    <updated>2019-06-10T17:48:43+08:00</updated>
    <id>http://isunix.github.io/blog/2019/06/10/emrzhong-chuang-jian-huede-superuser</id>
    <content type="html"><![CDATA[<p>想要在 EMR 中创建 HUE 的 superuser， 可以使用如下的方式</p>

<p><code>sh
cd /usr/lib/hue/
sudo build/env/bin/hue  createsuperuser
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Presto问题debug]]></title>
    <link href="http://isunix.github.io/blog/2019/06/04/prestowen-ti-debug/"/>
    <updated>2019-06-04T10:27:51+08:00</updated>
    <id>http://isunix.github.io/blog/2019/06/04/prestowen-ti-debug</id>
    <content type="html"><![CDATA[<p>我们在执行presto-cli命名的时候，可以使用如下的方式 <code>presto-cli --catalog hive --schema $schema --output-format CSV_HEADER --server $ip:$port --debug</code></p>

<p>这样就会打印出诊断信息出来.</p>

<p>我们使用如下的命令看下presto-server的状态 <code>initctl list | grep -i presto</code></p>

<p><code>sudo stop presto-server</code> 这样来关闭 presto-server； <code>sudo start presto-server</code> 这样来开启.</p>

<p>如果想要看查看presto的log，可以去 <code>/var/log/presto</code>, 对于debug非常有用.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PySpark中引入col函数的方式]]></title>
    <link href="http://isunix.github.io/blog/2019/06/04/pysparkzhong-yin-ru-colhan-shu-de-fang-shi/"/>
    <updated>2019-06-04T08:59:32+08:00</updated>
    <id>http://isunix.github.io/blog/2019/06/04/pysparkzhong-yin-ru-colhan-shu-de-fang-shi</id>
    <content type="html"><![CDATA[<p>在python代码中通过 <code>from pyspark.sql.functions import col</code> 来引入 <code>col</code> 这个函数的时候，总是报错，找不到这个函数. 后来参考
<a href="https://stackoverflow.com/questions/40163106/cannot-find-col-function-in-pyspark">https://stackoverflow.com/questions/40163106/cannot-find-col-function-in-pyspark</a> 这个文章, <code>pip install pyspark-stubs</code> 再去引用就好了.</p>

]]></content>
  </entry>
  
</feed>
