
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>PySpark的一些介绍 - Steven's Blog</title>
  <meta name="author" content="Steven Sun">

  
  <meta name="description" content="一些概念 Spark, PySpark, Spark SQL, SparkML Jobs, Stages, Task, Driver, Worker RDD, DataFrames, DataSets SparkSession RDD two ways to create an RDD in &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://isunix.github.io/blog/2019/08/21/pysparkde-%5B%3F%5D-xie-jie-shao">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Steven's Blog" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
  

</head>

<body    class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">Steven's Blog</a></h1>
  
    <h2>A Dream Land of Peace!</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:isunix.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about" class="nav-link">About Me</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">PySpark的一些介绍</h1>
    
    
      <p class="meta">
        








  


<time datetime="2019-08-21T08:00:24+08:00" pubdate data-updated="true">2019 年8 月21 日</time>
        
      </p>
    
  </header>


<div class="entry-content"><ol>
  <li>
    <h5 id="section">一些概念</h5>

    <ul>
      <li>Spark, PySpark, Spark SQL, SparkML</li>
      <li>Jobs, Stages, Task, Driver, Worker</li>
      <li>RDD, DataFrames, DataSets</li>
      <li>SparkSession</li>
    </ul>
  </li>
  <li>
    <h5 id="rdd">RDD</h5>

    <ol>
      <li>
        <p>two ways to create an RDD in PySpark</p>

        <p><code>python
data = sc.parallelize([('Amber', 22), ('Albert', 12), ('Amber', 9)])
</code></p>

        <p><code>python
data_from_file = sc.textFile('/data/PySpark_Data/VS14MORT.txt.gz', 4)
</code></p>
      </li>
      <li>
        <p>RDDs are <em>schema-less</em> data structures  Thus, parallelizing a dataset is perfectly fine with Spark when using RDDs:</p>
      </li>
      <li>
        <p>&#8220;`python
# parallelize a rdd
data_heterogenous = sc.parallelize([ 
  (‘Ferrari’, ‘fast’),
	{‘Porsche’: 100000}, 
  [‘Spain’,’visited’, 4504]
]).collect()</p>

        <h1 id="access-element">access element</h1>
        <p>data_heterogenous[1][‘Porsche’]
&#8220;`</p>
      </li>
      <li>Transformations
        <ol>
          <li>
            <p>.map(…)</p>
          </li>
          <li>
            <p>.filter(…)  </p>
          </li>
          <li>
            <p>.flatMap(…) </p>
          </li>
          <li>
            <p>.distinct(…)</p>
          </li>
          <li>
            <p>.sample(…)</p>
          </li>
          <li>
            <p>.leftOuterJoin(…)</p>
          </li>
          <li>
            <p>.repartition(…)</p>
          </li>
        </ol>
      </li>
      <li>Actions
        <ol>
          <li>.take(…)</li>
          <li>.reduce(…)</li>
          <li>.reduceByKey(…)</li>
          <li>.count()</li>
          <li>.collect()</li>
          <li>.saveAsTextFile(…)</li>
          <li>.foreach(…)</li>
        </ol>
      </li>
    </ol>
  </li>
  <li>
    <h5 id="dataframes">DataFrames</h5>

    <ol>
      <li>
        <p>specify the schema</p>

        <p>&#8220;`python
from pyspark.sql.types import *</p>

        <h1 id="generate-our-own-csv-data">Generate our own CSV data</h1>
        <p>#   This way we don’t have to access the file system yet.
stringCSVRDD = sc.parallelize([(123, ‘Katie’, 19, ‘brown’), (234, ‘Michael’, 22, ‘green’), (345, ‘Simone’, 23, ‘blue’)])</p>

        <h1 id="the-schema-is-encoded-in-a-string-using-structtype-we-define-the-schema-using-various-pysparksqltypes">The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types</h1>
        <p>schemaString = “id name age eyeColor”
schema = StructType([
    StructField(“id”, LongType(), True),  <br />
    StructField(“name”, StringType(), True),
    StructField(“age”, LongType(), True),
    StructField(“eyeColor”, StringType(), True)
])</p>

        <h1 id="apply-the-schema-to-the-rdd-and-create-dataframe">Apply the schema to the RDD and Create DataFrame</h1>
        <p>swimmers = spark.createDataFrame(stringCSVRDD, schema)</p>

        <h1 id="creates-a-temporary-view-using-the-dataframe">Creates a temporary view using the DataFrame</h1>
        <p>swimmers.createOrReplaceTempView(“swimmers”)
&#8220;`</p>
      </li>
      <li>
        <p>DataFrame Query</p>

        <p>&#8220;`python
# Set File Paths
flightPerfFilePath = “/databricks-datasets/flights/departuredelays.csv”
airportsFilePath = “/databricks-datasets/flights/airport-codes-na.txt”</p>

        <h1 id="obtain-airports-dataset">Obtain Airports dataset</h1>
        <p>airports = spark.read.csv(airportsFilePath, header=’true’, inferSchema=’true’, sep=’\t’)
airports.createOrReplaceTempView(“airports”)</p>

        <h1 id="obtain-departure-delays-dataset">Obtain Departure Delays dataset</h1>
        <p>flightPerf = spark.read.csv(flightPerfFilePath, header=’true’)
flightPerf.createOrReplaceTempView(“FlightPerformance”)</p>

        <h1 id="cache-the-departure-delays-dataset">Cache the Departure Delays dataset</h1>
        <p>flightPerf.cache()</p>

        <h1 id="query-sum-of-flight-delays-by-city-and-origin-code-for-washington-state">Query Sum of Flight Delays by City and Origin Code (for Washington State)</h1>
        <p>spark.sql(“select a.City, f.origin, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.State = ‘WA’ group by a.City, f.origin order by sum(f.delay) desc”).show()
&#8220;`</p>
      </li>
    </ol>
  </li>
  <li>
    <p>Prepare Data</p>

    <ol>
      <li>
        <p>create data frame</p>

        <p><code>python
df = spark.createDataFrame([
(1, 144.5, 5.9, 33, 'M'), (2, 167.2, 5.4, 45, 'M'), (3, 124.1, 5.2, 23, 'F'), (4, 144.5, 5.9, 33, 'M'), (5, 133.2, 5.7, 54, 'F'), (3, 124.1, 5.2, 23, 'F'), (5, 129.2, 5.3, 42, 'M'), ], ['id', 'weight', 'height',
'age', 'gender'])
</code></p>
      </li>
      <li>
        <p>get basic description</p>

        <p><code>python
print('Count of rows: {0}'.format(df.count()))
print('Count of distinct rows: {0}'.format(df.distinct().count()))
df = df.dropDuplicates()
df = df.dropDuplicates(subset=[c for c in df.columns if c != 'id'])
df.show()
</code></p>
      </li>
      <li>
        <p>To calculate the total and distinct number of IDs in one step we can use the <code>.agg(...)</code> method.</p>

        <p>&#8220;`python
import pyspark.sql.functions as fn</p>

        <p>df.agg(
    fn.count(‘id’).alias(‘count’),
    fn.countDistinct(‘id’).alias(‘distinct’)
).show()</p>

        <h1 id="give-each-row-a-unique-id">Give each row a unique ID.</h1>
        <p>df.withColumn(‘new_id’, fn.monotonically_increasing_id()).show()
&#8220;`</p>
      </li>
      <li>
        <p>missing observations</p>

        <p>&#8220;`python
df_miss = spark.createDataFrame([
        (1, 143.5, 5.6, 28,   ‘M’,  100000),
        (2, 167.2, 5.4, 45,   ‘M’,  None),
        (3, None , 5.2, None, None, None),
        (4, 144.5, 5.9, 33,   ‘M’,  None),
        (5, 133.2, 5.7, 54,   ‘F’,  None),
        (6, 124.1, 5.2, None, ‘F’,  None),
        (7, 129.2, 5.3, 42,   ‘M’,  76000),
    ], [‘id’, ‘weight’, ‘height’, ‘age’, ‘gender’, ‘income’])</p>

        <h1 id="to-find-the-number-of-missing-observations-per-row">To find the number of missing observations per row</h1>
        <p>df_miss.rdd.map(
    lambda row: (row[‘id’], sum([c == None for c in row]))
).collect()</p>

        <h1 id="see-what-values-are-missing">see what values are missing</h1>
        <p>df_miss.where(‘id == 3’).show()</p>

        <h1 id="get-the-percentage-of-missing-observations-we-see-in-each-column">get the percentage of missing observations we see in each column</h1>
        <p>df_miss.agg(<em>[
    (1 - (fn.count(c) / fn.count(‘</em>’))).alias(c + ‘_missing’)
    for c in df_miss.columns
]).show()</p>

        <h1 id="drop-the-income-feature-as-most-of-its-values-are-missing">drop the ‘income’ feature as most of its values are missing</h1>
        <p>df_miss_no_income = df_miss.select([c for c in df_miss.columns if c != ‘income’])
df_miss_no_income.show()</p>

        <h1 id="impute">impute</h1>
        <p>means = df_miss_no_income.agg(
    *[fn.mean(c).alias(c) for c in df_miss_no_income.columns if c != ‘gender’]
).toPandas().to_dict(‘records’)[0]</p>

        <p>means[‘gender’] = ‘missing’</p>

        <p>df_miss_no_income.fillna(means).show()</p>

        <h1 id="descriptive-statistics">Descriptive statistics</h1>
        <p>import pyspark.sql.types as typ
fraud = sc.textFile(‘ccFraud.csv.gz’)
header = fraud.first()</p>

        <p>fraud = fraud \
    .filter(lambda row: row != header) \
    .map(lambda row: [int(elem) for elem in row.split(‘,’)])</p>

        <p>fields = [
    *[
        typ.StructField(h[1:-1], typ.IntegerType(), True)
        for h in header.split(‘,’)
    ]
]</p>

        <p>schema = typ.StructType(fields)</p>

        <p>fraud_df = spark.createDataFrame(fraud, schema)
fraud_df.printSchema()
fraud_df.groupby(‘gender’).count().show()</p>

        <p>numerical = [‘balance’, ‘numTrans’, ‘numIntlTrans’]
desc = fraud_df.describe(numerical)
desc.show()</p>

        <p>fraud_df.agg({‘balance’: ‘skewness’}).show()</p>

        <h1 id="find-correlations">find Correlations</h1>
        <p>fraud_df.corr(‘balance’, ‘numTrans’)</p>

        <h1 id="to-create-a-correlations-matrix">to create a correlations matrix</h1>
        <p>n_numerical = len(numerical)</p>

        <p>corr = []</p>

        <p>for i in range(0, n_numerical):
    temp = [None] * i</p>

        <pre><code>for j in range(i, n_numerical):
    temp.append(fraud_df.corr(numerical[i], numerical[j]))
corr.append(temp)
</code></pre>

        <p>corr
&#8220;`</p>
      </li>
    </ol>
  </li>
</ol>

</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Steven Sun</span></span>

      








  


<time datetime="2019-08-21T08:00:24+08:00" pubdate data-updated="true">2019 年8 月21 日</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/data-and-ml-and-ai/'>Data&ML&AI</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2019/08/14/data-science-at-the-command-line/" title="Previous Post: Data Science at the Command Line">&laquo; Data Science at the Command Line</a>
      
      
        <a class="basic-alignment right" href="/blog/2019/08/27/sparkzhong-pai-xu-shu-chu/" title="Next Post: Spark中排序输出">Spark中排序输出 &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2019/08/27/ru-he-cha-kan-linuxzhong-de-nei-cun-zhan-yong-qing-kuang/">如何查看Linux中的内存占用情况</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/08/27/sparkzhong-pai-xu-shu-chu/">Spark中排序输出</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/08/21/pysparkde-%5B%3F%5D-xie-jie-shao/">PySpark的一些介绍</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/08/14/data-science-at-the-command-line/">Data Science at the Command Line</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/08/14/gradient-boosting-algorithm/">Gradient Boosting Algorithm</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/08/07/guan-yu-pzhi/">关于P值</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/08/01/shi-yong-sparkde-mllibjin-xing-ji-qi-xue-xi/">使用Spark的MLlib进行机器学习</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/07/30/macshang-dan-ji-qi-pysparkyu-dao-de-wen-ti-debug/">Mac上单机起PySpark遇到的问题debug</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/07/29/pythonzhong-bu-shu-chu-warningde-xin-xi/">Python中不输出warning的信息</a>
      </li>
    
      <li class="post">
        <a href="/blog/2019/07/24/vscode-remote-yuan-cheng-kai-fa-yu-diao-shi/">VScode Remote 远程开发与调试</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>Categories</h1>
  <ul id="categories">
    <li class='category'><a href='/blog/categories/algorithms/'>Algorithms (8)</a></li>
<li class='category'><a href='/blog/categories/bigdata/'>BigData (11)</a></li>
<li class='category'><a href='/blog/categories/blog/'>Blog (6)</a></li>
<li class='category'><a href='/blog/categories/c/'>C (4)</a></li>
<li class='category'><a href='/blog/categories/data-and-ml-and-ai/'>Data&ML&AI (12)</a></li>
<li class='category'><a href='/blog/categories/django-and-flask/'>Django&Flask (5)</a></li>
<li class='category'><a href='/blog/categories/editor/'>Editor (2)</a></li>
<li class='category'><a href='/blog/categories/fun/'>Fun (1)</a></li>
<li class='category'><a href='/blog/categories/javascript/'>Javascript (4)</a></li>
<li class='category'><a href='/blog/categories/linux-and-mac/'>Linux&Mac (14)</a></li>
<li class='category'><a href='/blog/categories/php/'>PHP (5)</a></li>
<li class='category'><a href='/blog/categories/perl/'>Perl (50)</a></li>
<li class='category'><a href='/blog/categories/python/'>Python (10)</a></li>
<li class='category'><a href='/blog/categories/ruby/'>Ruby (1)</a></li>
<li class='category'><a href='/blog/categories/scala/'>Scala (1)</a></li>
<li class='category'><a href='/blog/categories/shell/'>Shell (12)</a></li>

  </ul>
</section>

  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2019 - Steven Sun -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  











</body>
</html>
